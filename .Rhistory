geom_text(aes(y = store_uscore, label = round(store_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 4, color = "black") +
geom_text(aes(y = meta_uscore, label = round(meta_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 4, color = "black") +
labs(x = "Genres", y = "Average Scores", title = "Store vs Meta User Scores by Genre") +
scale_fill_manual(name = "Score Type", values = c("Store User Score" = "skyblue", "Meta User Score" = "salmon")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme(legend.position = "top")
# Load the dataset (replace with your file path)
steam_data <- readRDS("~/OneDrive/BIOS580/steamdb.rds")
# Split genres into separate rows and filter relevant columns
steam_data_expanded <- steam_data %>%
separate_rows(genres, sep = ",") %>%
select(genres, store_uscore, meta_uscore) %>%
filter(!is.na(store_uscore) & !is.na(meta_uscore))
# Convert the user scores to numeric (handling errors)
steam_data_expanded$store_uscore <- as.numeric(steam_data_expanded$store_uscore)
steam_data_expanded$meta_uscore <- as.numeric(steam_data_expanded$meta_uscore)
# Calculate the mean scores by genre
genre_scores <- steam_data_expanded %>%
group_by(genres) %>%
summarise(store_uscore = mean(store_uscore, na.rm = TRUE),
meta_uscore = mean(meta_uscore, na.rm = TRUE)) %>%
filter(!is.na(store_uscore) & !is.na(meta_uscore))
# Create the plot
ggplot(genre_scores, aes(x = genres)) +
geom_bar(aes(y = store_uscore, fill = "Store User Score"), stat = "identity", position = "dodge", width = 0.7) +
geom_bar(aes(y = meta_uscore, fill = "Meta User Score"), stat = "identity", position = "dodge", width = 0.7) +
geom_text(aes(y = store_uscore, label = round(store_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 3, color = "black") +
geom_text(aes(y = meta_uscore, label = round(meta_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 3, color = "black") +
labs(x = "Genres", y = "Average Scores", title = "Store vs Meta User Scores by Genre") +
scale_fill_manual(name = "Score Type", values = c("Store User Score" = "skyblue", "Meta User Score" = "salmon")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme(legend.position = "top")
knitr::opts_chunk$set(echo = TRUE)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(tidyr)
# Load the dataset (replace with your file path)
steam_data <- readRDS("~/OneDrive/BIOS580/steamdb.rds")
# Split genres into separate rows and filter relevant columns
steam_data_expanded <- steam_data %>%
separate_rows(genres, sep = ",") %>%
select(genres, store_uscore, meta_uscore) %>%
filter(!is.na(store_uscore) & !is.na(meta_uscore))
# Convert the user scores to numeric (handling errors)
steam_data_expanded$store_uscore <- as.numeric(steam_data_expanded$store_uscore)
steam_data_expanded$meta_uscore <- as.numeric(steam_data_expanded$meta_uscore)
# Calculate the mean scores by genre
genre_scores <- steam_data_expanded %>%
group_by(genres) %>%
summarise(store_uscore = mean(store_uscore, na.rm = TRUE),
meta_uscore = mean(meta_uscore, na.rm = TRUE)) %>%
filter(!is.na(store_uscore) & !is.na(meta_uscore))
# Create the plot
ggplot(genre_scores, aes(x = genres)) +
geom_bar(aes(y = store_uscore, fill = "Store User Score"), stat = "identity", position = "dodge", width = 0.7) +
geom_bar(aes(y = meta_uscore, fill = "Meta User Score"), stat = "identity", position = "dodge", width = 0.7) +
geom_text(aes(y = store_uscore, label = round(store_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 2.5, color = "black") +
geom_text(aes(y = meta_uscore, label = round(meta_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 2.5, color = "black") +
labs(x = "Genres", y = "Average Scores", title = "Store vs Meta User Scores by Genre") +
scale_fill_manual(name = "Score Type", values = c("Store User Score" = "skyblue", "Meta User Score" = "salmon")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme(legend.position = "top")
# Load the dataset (replace with your file path)
steam_data <- readRDS("~/OneDrive/BIOS580/steamdb.rds")
# Split genres into separate rows and filter relevant columns
steam_data_expanded <- steam_data %>%
separate_rows(genres, sep = ",") %>%
select(genres, store_uscore, meta_uscore) %>%
filter(!is.na(store_uscore) & !is.na(meta_uscore))
# Convert the user scores to numeric (handling errors)
steam_data_expanded$store_uscore <- as.numeric(steam_data_expanded$store_uscore)
steam_data_expanded$meta_uscore <- as.numeric(steam_data_expanded$meta_uscore)
# Calculate the mean scores by genre
genre_scores <- steam_data_expanded %>%
group_by(genres) %>%
summarise(store_uscore = mean(store_uscore, na.rm = TRUE),
meta_uscore = mean(meta_uscore, na.rm = TRUE)) %>%
filter(!is.na(store_uscore) & !is.na(meta_uscore))
# Create the plot
ggplot(genre_scores, aes(x = genres)) +
geom_bar(aes(y = store_uscore, fill = "Store User Score"), stat = "identity", position = "dodge", width = 0.7) +
geom_bar(aes(y = meta_uscore, fill = "Meta User Score"), stat = "identity", position = "dodge", width = 0.7) +
geom_text(aes(y = store_uscore, label = round(store_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 2.5, color = "black") +
geom_text(aes(y = meta_uscore, label = round(meta_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 2.5, color = "black") +
labs(x = "Genres", y = "Average Scores", title = "Store vs Meta User Scores by Genre") +
scale_fill_manual(name = "Score Type", values = c("Store User Score" = "skyblue", "Meta User Score" = "salmon")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme(legend.position = "top")
ggsave("Group7_jhfso.png", width = 8, height = 6, dpi = 300)
# Load the dataset (replace with your file path)
steam_data <- readRDS("~/OneDrive/BIOS580/steamdb.rds")
# Split genres into separate rows and filter relevant columns
steam_data_expanded <- steam_data %>%
separate_rows(genres, sep = ",") %>%
select(genres, store_uscore, meta_uscore) %>%
filter(!is.na(store_uscore) & !is.na(meta_uscore))
# Convert the user scores to numeric (handling errors)
steam_data_expanded$store_uscore <- as.numeric(steam_data_expanded$store_uscore)
steam_data_expanded$meta_uscore <- as.numeric(steam_data_expanded$meta_uscore)
# Calculate the mean scores by genre
genre_scores <- steam_data_expanded %>%
group_by(genres) %>%
summarise(store_uscore = mean(store_uscore, na.rm = TRUE),
meta_uscore = mean(meta_uscore, na.rm = TRUE)) %>%
filter(!is.na(store_uscore) & !is.na(meta_uscore))
# Create the plot
ggplot(genre_scores, aes(x = genres)) +
geom_bar(aes(y = store_uscore, fill = "Store User Score"), stat = "identity", position = "dodge", width = 0.7) +
geom_bar(aes(y = meta_uscore, fill = "Meta User Score"), stat = "identity", position = "dodge", width = 0.7) +
geom_text(aes(y = store_uscore, label = round(store_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 2.5, color = "black") +
geom_text(aes(y = meta_uscore, label = round(meta_uscore, 1)),
position = position_dodge(width = 0.5), vjust = -0.5, size = 2.5, color = "black") +
labs(x = "Genres", y = "Average Scores", title = "Store vs Meta User Scores by Genre") +
scale_fill_manual(name = "Score Type", values = c("Store User Score" = "skyblue", "Meta User Score" = "salmon")) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
theme(legend.position = "top")+
theme(panel.background = element_rect(fill = "white"),  # 设置面板背景为白色
plot.background = element_rect(fill = "white"),   # 设置绘图背景为白色
panel.grid.major = element_line(color = "gray90"), # 主要网格线颜色（可以调为灰色或其他颜色）
panel.grid.minor = element_blank())  # 移除次要网格线
ggsave("Group7_jhfso.png", width = 8, height = 6, dpi = 300)
hiv_cases = read.csv('AtlasPlusTableData_HIV_2016.csv')
socdet = read.csv('AtlasPlusTableData_SocialDeterminants_2016.csv')
library(tidyr)
socdet_wide = socdet%>%pivot_wider(names_from = Indicator, values_from=Percent)
dat = merge(hiv_cases,socdet_wide)
dat$Pop100k = dat$Cases/dat$Rate.per.100000
hiv_cases = read.csv('AtlasPlusTableData_HIV_2016.csv')
socdet = read.csv('AtlasPlusTableData_SocialDeterminants_2016.csv')
library(tidyr)
socdet_wide = socdet%>%pivot_wider(names_from = Indicator, values_from=Percent)
dat = merge(hiv_cases,socdet_wide)
dat$Pop100k = dat$Cases/dat$Rate.per.100000
View(dat)
View(dat)
hiv_cases = read.csv('AtlasPlusTableData_HIV_2016.csv')
socdet = read.csv('AtlasPlusTableData_SocialDeterminants_2016.csv')
library(tidyr)
socdet_wide = socdet%>%pivot_wider(names_from = Indicator, values_from=Percent)
dat = merge(hiv_cases,socdet_wide)
dat$Pop100k = dat$Cases/dat$Rate.per.100000
# Fit the intercept-only model
intercept_only <- lm(Cases ~ 1, data=dat)
# View the model summary to interpret the intercept
summary(intercept_only)
View(dat)
# Fit the intercept-only Poisson model
poisson_intercept <- glm(Cases ~ 1, family = poisson(link = "log"), data = dat)
summary(poisson_intercept)
View(dat)
hiv_cases = read.csv('AtlasPlusTableData_HIV_2016.csv')
socdet = read.csv('AtlasPlusTableData_SocialDeterminants_2016.csv')
library(tidyr)
socdet_wide = socdet%>%pivot_wider(names_from = Indicator, values_from=Percent)
dat = merge(hiv_cases,socdet_wide)
dat$Pop100k = dat$Cases/dat$Rate.per.100000
# Fit the intercept-only model
lm_intercept <- lm(Cases ~ 1, data=dat)
summary(lm_intercept)
lm_var <- summary(lm_intercept)$sigma^2
mu_hat <- fitted(poisson_intercept)
poisson_var <- mu_hat
print(lm_var)
print(poisson_var)
lm_var <- summary(lm_intercept)$sigma^2
mu_hat <- fitted(poisson_intercept)
poisson_var <- mu_hat
print(lm_var)
#print(poisson_var)
lm_var <- summary(lm_intercept)$sigma^2
mu_hat <- fitted(poisson_intercept)
poisson_var <- mu_hat
print(lm_var)
print(poisson_var)
# Fit the intercept-only quasi-Poisson model
quasipoisson_intercept <- glm(Cases ~ 1, family = quasipoisson(link = "log"), data = dat)
summary(quasipoisson_intercept)
# Expected counts
expected_counts_quasipoisson <- exp(coef(quasi_poisson_model)[1])
# Fit the intercept-only quasi-Poisson model
quasipoisson_intercept <- glm(Cases ~ 1, family = quasipoisson(link = "log"), data = dat)
summary(quasipoisson_intercept)
# Expected counts
expected_counts_quasipoisson <- exp(coef(quasipoisson_intercept)[1])
expected_counts_quasi_poisson
# Fit the intercept-only quasi-Poisson model
quasipoisson_intercept <- glm(Cases ~ 1, family = quasipoisson(link = "log"), data = dat)
summary(quasipoisson_intercept)
# Expected counts
expected_counts_quasipoisson <- exp(coef(quasipoisson_intercept)[1])
expected_counts_quasipoisson
# Fit the intercept-only Poisson model
poisson_intercept <- glm(Cases ~ 1, family = poisson(link = "log"), data = dat)
summary(poisson_intercept)
# Back-transformed scale
expected_counts_poisson = exp(coef(poisson_intercept)[1])
# Calculate the dispersion parameter
dispersion_parameter <- summary(quasipoisson_intercept)$dispersion
dispersion_parameter
# Fit the intercept-only OLS model
ols_log_intercept <- lm(log(Rate.per.100000) ~ 1, data = dat)
summary(ols_log_intercept)
# Back-transform the intercept
intercept_back_transformed <- exp(coef(ols_log_intercept)[1])
# Display the results
intercept_back_transformed
# Fit the Poisson model with offset log(Pop100k)
poisson_offset_model <- glm(Cases ~ 1 + offset(log(Pop100k)), family = poisson(link = "log"), data = dat)
summary(poisson_offset_model)
# Back-transform the intercept
intercept_back_transformed_offset <- exp(coef(poisson_offset_model)[1])
intercept_back_transformed_offset
# Fit the Poisson model with offset log(Pop100k) and centered predictors
dat$Poverty_centered <- dat$`Households living below the federal poverty level` - mean(dat$`Households living below the federal poverty level`, na.rm = TRUE)
dat$NoHS_centered <- dat$`Population 25 years and older w/o HS diploma` - mean(dat$`Population 25 years and older w/o HS diploma`, na.rm = TRUE)
dat$Uninsured_centered <- dat$Uninsured - mean(dat$Uninsured, na.rm = TRUE)
dat$VacantHousing_centered <- dat$`Vacant housing` - mean(dat$`Vacant housing`, na.rm = TRUE)
# Fit the Poisson model
poisson_model_centered <- glm(Cases ~ Poverty_centered + NoHS_centered + Uninsured_centered + VacantHousing_centered + offset(log(Pop100k)),
family = poisson(link = "log"), data = dat)
# Summary of the model
summary(poisson_model_centered)
# Pearson Chi-Square Test
pearson_chi_sq <- sum(residuals(poisson_model_centered, type = "pearson")^2)
p_value_pearson <- 1 - pchisq(pearson_chi_sq, df = 46)
p_value_pearson
# Pearson Chi-Square Test
pearson_chi_sq <- sum(residuals(poisson_model_centered, type = "pearson")^2)
p_value_pearson <- 1 - pchisq(pearson_chi_sq, df = 46)
p_value_pearson
pearson_chi_sq
# Pearson Chi-Square Test
pearson_chi_sq <- sum(residuals(poisson_model_centered, type = "pearson")^2)
p_value_pearson <- 1 - pchisq(5597, df = 46)
p_value_pearson
pearson_chi_sq
# Pearson Chi-Square Test
pearson_chi_sq <- sum(residuals(poisson_model_centered, type = "pearson")^2)
p_value_pearson <- 1 - pchisq(1, df = 46)
p_value_pearson
pearson_chi_sq
# Pearson Chi-Square Test
pearson_chi_sq <- sum(residuals(poisson_model_centered, type = "pearson")^2)
p_value_pearson <- 1 - pchisq(124, df = 46)
p_value_pearson
pearson_chi_sq
# Pearson Chi-Square Test
pearson_chi_sq <- sum(residuals(poisson_model_centered, type = "pearson")^2)
p_value_pearson <- 1 - pchisq(1244, df = 46)
p_value_pearson
pearson_chi_sq
# Check for overdispersion
dispersion_parameter <- pearson_chi_sq / 46
dispersion_parameter
# Fit the quasi-Poisson model
quasi_poisson_model <- glm(Cases ~ Poverty_centered + NoHS_centered + Uninsured_centered + VacantHousing_centered + offset(log(Pop100k)),
family = quasipoisson(link = "log"), data = dat)
summary(quasi_poisson_model)
# Extract the overdispersion parameter
overdispersion_parameter <- summary(quasi_poisson_model)$dispersion
overdispersion_parameter
# Create diagnostic plots
par(mfrow = c(2, 2))  # Arrange plots in a 2x2 layout
plot(quasi_poisson_model)
# Intercept
intercept_log_quasipoisson <- coef(quasipoisson_model)[1]
# Fit the quasi-Poisson model
quasipoisson_model <- glm(Cases ~ Poverty_centered + NoHS_centered + Uninsured_centered + VacantHousing_centered + offset(log(Pop100k)),
family = quasipoisson(link = "log"), data = dat)
summary(quasipoisson_model)
# Extract the overdispersion parameter
overdispersion_parameter <- summary(quasipoisson_model)$dispersion
overdispersion_parameter
# Create diagnostic plots
par(mfrow = c(2, 2))
plot(quasipoisson_model)
# Intercept
intercept_log_quasipoisson <- coef(quasipoisson_model)[1]
# Back-transform the intercept
intercept_back_transformed_quasipoisson <- exp(intercept_log_quasipoisson)
# Log_SE
log_se <- summary(quasipoisson_model)$coefficients["(Intercept)", "Std. Error"]
log_lower_quasipoisson <- intercept_log_quasipoisson - 1.96 * log_se
log_upper_quasipoisson <- intercept_log_quasipoisson + 1.96 * log_se
lower_back_transformed_quasipoisson <- exp(log_lower_quasipoisson)
upper_back_transformed_quasipoisson <- exp(log_upper_quasipoisson)
cat("Back-transformed intercept:", intercept_back_transformed_quasipoisson, "\n")
cat("95% Confidence Interval for the back-transformed intercept: [",
lower_back_transformed_quasipoisson, ",", upper_back_transformed_quasipoisson, "]\n")
# Fit the log-linear model with Gaussian assumptions
log_linear_model <- lm(log(Rate.per.100000) ~ Poverty_centered + NoHS_centered + Uninsured_centered + VacantHousing_centered, data = dat)
summary(log_linear_model)
# Assess model
par(mfrow = c(2, 2))
plot(log_linear_model)
malawi = read.csv('Malawi_Fecal_Sludge_Data_to_post.csv')
View(malawi)
View(malawi)
unique(malawi$Latrine_ID)
unique(malawi$Depth)
unique(malawi$HH_population)
unique(malawi$shared_latrine)
unique(malawi$pit_type)
unique(malawi$water_source)
unique(malawi$toilet_paper)
library(lme4)
logit_model <- glmer(Depth_pathogen_presence ~ Depth + HH_population + shared_latrine + pit_type + water_source + toilet_paper +
(1 | Latrine_ID) + (1 | Pathogen_Target) + (1 | Latrine_ID:Pathogen_Target),
data = malawi,
family = binomial(link = "logit")
)
summary(logit_model)
library(lme4Test)
install.packages("lme4Test")
# Fit the model using all available optimizers
logit_model_all_fit <- allFit(logit_model)
# Check convergence messages
allfits <- logit_model_all_fit
lapply(allfits, function(x) x@optinfo$conv$lme4$messages)
lapply(allfits, print)
# Extract the bobyqa results
model.bobyqa <- allfits[['bobyqa']]
summary(model.bobyqa)
# Extract the coefficient and standard error for water_sourcepublic
coef_water_sourcepublic <- summary_bobyqa$coefficients["water_sourcepublic", "Estimate"]
# Extract the coefficient and standard error for water_sourcepublic
coef_water_sourcepublic <- summary(model.bobyqa)$coefficients["water_sourcepublic", "Estimate"]
se_water_sourcepublic <- summary(model.bobyqa)$coefficients["water_sourcepublic", "Std. Error"]
# Calculate the odds ratio
odds_ratio_public_vs_compound <- exp(beta_water_sourcepublic)
# Extract the coefficient and standard error for water_sourcepublic
coef_water_sourcepublic <- summary(model.bobyqa)$coefficients["water_sourcepublic", "Estimate"]
se_water_sourcepublic <- summary(model.bobyqa)$coefficients["water_sourcepublic", "Std. Error"]
# Calculate the odds ratio
odds_ratio_public_vs_compound <- exp(coef_water_sourcepublic)
# Calculate the 95% confidence interval
lower_bound_sourcepublic <- coef_water_sourcepublic - 1.96 * se_water_source_public
# Extract the coefficient and standard error for water_sourcepublic
coef_water_sourcepublic <- summary(model.bobyqa)$coefficients["water_sourcepublic", "Estimate"]
se_water_sourcepublic <- summary(model.bobyqa)$coefficients["water_sourcepublic", "Std. Error"]
# Calculate the odds ratio
odds_ratio_public_vs_compound <- exp(coef_water_sourcepublic)
# Calculate the 95% confidence interval
lower_bound_sourcepublic <- coef_water_sourcepublic - 1.96 * se_water_sourcepublic
upper_bound_sourcepublic <- coef_water_sourcepublic + 1.96 * se_water_sourcepublic
# Convert the confidence interval for the coefficient into odds ratio terms
lower_bound_or <- exp(lower_bound_sourcepublic)
upper_bound_or <- exp(upper_bound_sourcepublic)
cat("Odds Ratio for public vs. compound water source:", odds_ratio_public_vs_compound, "\n")
cat("95% Confidence Interval for Odds Ratio: [", lower_bound_or, ", ", upper_bound_or, "]\n")
# Extract the coefficients from the model summary
coef_Intercept <- summary(model.bobyqa)$coefficients["(Intercept)", "Estimate"]
coef_HH_population <- summary(model.bobyqa)$coefficients["HH_population", "Estimate"]
coef_water_sourcepublic <- summary(model.bobyqa)$coefficients["water_sourcepublic", "Estimate"]
# Set values for the predictors
HH_population <- 10
water_sourcepublic <- 1
# Calculate the log-odds of pathogen presence
log_odds <- coef_Intercept +
coef_HH_population * HH_population +
coef_water_sourcepublic * water_source_public
# Extract the coefficients from the model summary
coef_Intercept <- summary(model.bobyqa)$coefficients["(Intercept)", "Estimate"]
coef_HH_population <- summary(model.bobyqa)$coefficients["HH_population", "Estimate"]
coef_water_sourcepublic <- summary(model.bobyqa)$coefficients["water_sourcepublic", "Estimate"]
# Set values for the predictors
HH_population <- 10
water_sourcepublic <- 1
# Calculate the log-odds of pathogen presence
log_odds <- coef_Intercept +
coef_HH_population * HH_population +
coef_water_sourcepublic * water_sourcepublic
# Convert log-odds to probability
probability <- 1 / (1 + exp(-log_odds))
cat("Probability of pathogen presence in a household with 10 people and a public water source:", probability, "\n")
lm_var <- summary(lm_intercept_model)$sigma^2
rmarkdown::pandoc_available("1.12.3")
?rmarkdown::pandoc_available
Sys.getenv("RSTUDIO_PANDOC")
Sys.which("pandoc")
Sys.getenv("RSTUDIO_PANDOC")
rmarkdown::find_pandoc()
load("~/OneDrive/24fall/bios522_survival/activity/whas500.RData")
knitr::opts_chunk$set(echo = TRUE)
# Load required libraries
library(survival)
library(survminer)
# Set up the survival object
# Here, 'lenfol' represents the total length of follow-up, and 'fstat' represents the vital status (0 = Alive, 1 = Dead)
surv_obj <- Surv(whas500$lenfol, whas500$fstat)
# List of covariates to analyze
covariates <- c("age", "gender", "hr", "sysbp", "diabp", "bmi", "cvd", "afb", "sho", "chf", "av3", "miord", "mitype", "year")
# Loop through each covariate to fit univariable models and plot Kaplan-Meier curves
for (covariate in covariates) {
# Fit the Cox proportional hazards model for each covariate
cox_model <- coxph(as.formula(paste("surv_obj ~", covariate)), data = whas500)
summary(cox_model) # This will provide details on the model
# Generate and save the Kaplan-Meier plot
km_fit <- survfit(as.formula(paste("surv_obj ~", covariate)), data = whas500)
ggsurvplot(km_fit, data = whas500,
pval = TRUE, # Show p-value
title = paste("Kaplan-Meier Curve for", covariate),
xlab = "Time in days",
ylab = "Survival Probability",
legend.title = covariate)
# Pause to view each plot or save them
# Uncomment the line below if you want to save each plot as a file
# ggsave(filename = paste0("KM_plot_", covariate, ".png"))
}
View(whas500)
# Load required libraries
library(survival)
library(survminer)
# Set up the survival object
# Here, 'lenfol' represents the total length of follow-up, and 'fstat' represents the vital status (0 = Alive, 1 = Dead)
surv_obj <- Surv(whas500$lenfol, whas500$fstat)
# List of covariates to analyze
covariates <- c("age", "gender", "hr", "sysbp", "diasbp", "bmi", "cvd", "afb", "sho", "chf", "av3", "miord", "mitype", "year")
# Loop through each covariate to fit univariable models and plot Kaplan-Meier curves
for (covariate in covariates) {
# Fit the Cox proportional hazards model for each covariate
cox_model <- coxph(as.formula(paste("surv_obj ~", covariate)), data = whas500)
summary(cox_model) # This will provide details on the model
# Generate and save the Kaplan-Meier plot
km_fit <- survfit(as.formula(paste("surv_obj ~", covariate)), data = whas500)
ggsurvplot(km_fit, data = whas500,
pval = TRUE, # Show p-value
title = paste("Kaplan-Meier Curve for", covariate),
xlab = "Time in days",
ylab = "Survival Probability",
legend.title = covariate)
# Pause to view each plot or save them
# Uncomment the line below if you want to save each plot as a file
# ggsave(filename = paste0("KM_plot_", covariate, ".png"))
}
# Load required libraries
library(survival)
library(survminer)
# Set up the survival object
surv_obj <- Surv(whas500$lenfol, whas500$fstat)
# List of covariates to analyze
covariates <- c("age", "gender", "hr", "sysbp", "diasbp", "bmi", "cvd", "afb", "sho", "chf", "av3", "miord", "mitype", "year")
# Loop through each covariate to fit univariable models and plot Kaplan-Meier curves
for (covariate in covariates) {
# Fit the Cox proportional hazards model for each covariate
cox_model <- coxph(as.formula(paste("surv_obj ~", covariate)), data = whas500)
print(summary(cox_model)) # Print the summary of the model for inspection
# Generate the Kaplan-Meier plot
km_fit <- survfit(as.formula(paste("surv_obj ~", covariate)), data = whas500)
plot_title <- paste("Kaplan-Meier Curve for", covariate)
ggsurvplot(km_fit, data = whas500,
pval = TRUE, # Show p-value
title = plot_title,
xlab = "Time in days",
ylab = "Survival Probability",
legend.title = covariate)
# Pause to view each plot in the RStudio viewer, or save the plots to files if preferred
# Uncomment the line below to save each plot as a PNG file
# ggsave(filename = paste0("KM_plot_", covariate, ".png"), plot = last_plot())
# Add a small delay if you want to manually review plots in RStudio Viewer
Sys.sleep(2) # Adjust as needed for viewing time between plots
}
significant_covariates <- c("age", "gender", "hr", "sysbp", "diasbp", "bmi", "afb", "sho", "chf", "miord", "mitype", "year")
multivariable_model <- coxph(as.formula(paste("surv_obj ~", paste(significant_covariates, collapse = " + "))), data = whas500)
summary(multivariable_model)  # View the summary of the multivariable model
# Step 2: Check for covariates to remove
# Stepwise selection using AIC for model simplification
stepwise_model <- step(multivariable_model, direction = "backward")
# Print the simplified model
summary(stepwise_model)
# Optionally, extract the final list of covariates in the simplified model
final_covariates <- names(coef(stepwise_model))
# Print final selected covariates
cat("Final selected covariates after model simplification:", final_covariates, "\n")
# ADD YOUR CODE HERE
# Fit the full model (assuming significant_covariates contains all significant covariates from Step 1)
full_model <- coxph(as.formula(paste("surv_obj ~", paste(significant_covariates, collapse = " + "))), data = whas500)
# Fit the reduced model (using final_covariates from Step 2's stepwise_model)
reduced_model <- coxph(as.formula(paste("surv_obj ~", paste(final_covariates, collapse = " + "))), data = whas500)
# Extract coefficients from both models
full_coefs <- coef(full_model)
reduced_coefs <- coef(reduced_model)
print(full_coefs)
print(reduced_coefs)
rmarkdown::render(
here::here("report.Rmd")
)
Sys.getenv("RSTUDIO_PANDOC")
renv::restore()
setwd(~/OneDrive/24fall/Data550-cy/module10_renv/renvPractice)
setwd("~/OneDrive/24fall/Data550-cy/module10_renv/renvPractice")
renv::restore()
renv::restore()
renv::restore()
renv::restore()
renv::restore()
renv::restore()
